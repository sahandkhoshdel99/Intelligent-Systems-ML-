{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s4TPKPLbgQI_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import math \n",
    "from numpy import linalg as LA\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from time import sleep\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wrryXSLy50uM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import sklearn.cluster\n",
    "import sklearn.metrics\n",
    "import matplotlib.cm as cm\n",
    "import itertools\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn import mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1jM2UjughtJ",
    "outputId": "340e3abc-f815-457d-d903-1721d80a6e99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38fffF-KXkoR"
   },
   "source": [
    "## Reading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOeMikgRgj4w"
   },
   "outputs": [],
   "source": [
    "toy_data = pd.read_csv('/content/drive/My Drive/IS/Final/ToyDataSet.csv').to_numpy()\n",
    "bboxes_data = pd.read_csv('/content/drive/My Drive/IS/Final/bboxes.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gfDhY7F6hK5o"
   },
   "outputs": [],
   "source": [
    "def normalize(x): # for normalizing datasets' features.\n",
    "    y = (x - np.mean(x, axis = 0))/np.std(x, axis = 0)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = pd.read_csv('ToyDataSet.csv').to_numpy()\n",
    "bboxes_data = pd.read_csv('bboxes.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "s7AhbHT1h0wd"
   },
   "outputs": [],
   "source": [
    "toy_data = normalize( toy_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0KfWGZtu7zUk"
   },
   "outputs": [],
   "source": [
    "norm_bboxes = np.zeros((bboxes_data.shape[0],2))\n",
    "# normalizing BBoexs with given formula to reduce dimensions.\n",
    "norm_bboxes[ :, 0 ] = (bboxes_data[:,4] - bboxes_data[:,2])/(bboxes_data[:,0])\n",
    "norm_bboxes[ :, 1 ] = (bboxes_data[:,5] - bboxes_data[:,3])/(bboxes_data[:,1])\n",
    "# Shuffling Dataset to have a well-distributed sampling.\n",
    "np.random.shuffle( norm_bboxes )\n",
    "np.random.shuffle( toy_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oi_V6CAHh9IJ"
   },
   "outputs": [],
   "source": [
    "class Agglomerative:\n",
    "    def __init__ ( self , n_clusters = 1 , affinity = 'euclidean' , linkage = 'single' ):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.affinity = affinity\n",
    "        self.linkage = linkage\n",
    "        self.clusters = {}\n",
    "        self.affinity_mat = 0\n",
    "        self.avg_silhoutte_score = []\n",
    "        self.assigned_labels_all = []\n",
    "\n",
    "    def cal_affinity( self , data ): # caluclates distance of each pair of points in dataset.\n",
    "        n = data.shape[ 0 ]\n",
    "        self.affinity_mat = np.zeros(( n , n ))\n",
    "        for first_data in range( n ):\n",
    "            for second_data in range( first_data , n ):\n",
    "                self.affinity_mat[ first_data , second_data ] = self.affinity_dist( data[ first_data , : ] , data[ second_data , : ] )\n",
    "                self.affinity_mat[ second_data , first_data ] = self.affinity_mat[ first_data , second_data ]\n",
    "\n",
    "    def affinity_dist( self , x , y ): # caclulates distance for 2 given points with 2 different method.\n",
    "        if self.affinity == 'euclidean' : \n",
    "            return LA.norm( x - y )\n",
    "        elif self.affinity == 'manhattan' : \n",
    "            return LA.norm( x - y , ord=1)\n",
    "\n",
    "    def cluster_distance( self , a , b ): # calculates distace of 2 clusters based on diffenrent methods\n",
    "        cluster_1 = self.clusters[ a ]     #  given as option in calss attributes.\n",
    "        cluster_2 = self.clusters[ b ]\n",
    "        if self.linkage == 'single':\n",
    "            temp = []\n",
    "            for data_1 in  cluster_1:\n",
    "                for data_2 in cluster_2:\n",
    "                    temp.append( self.affinity_mat[ data_1 , data_2 ] )\n",
    "            return np.min( temp )\n",
    "        elif self.linkage == 'complete':\n",
    "            temp = []\n",
    "            for data_1 in  cluster_1:\n",
    "                for data_2 in cluster_2:\n",
    "                    temp.append( self.affinity_mat[ data_1 , data_2 ] )\n",
    "            return np.max( temp )\n",
    "        elif self.linkage == 'average':\n",
    "            temp = []\n",
    "            for data_1 in  cluster_1:\n",
    "                for data_2 in cluster_2:\n",
    "                    temp.append( self.affinity_mat[ data_1 , data_2 ] )\n",
    "            return np.mean( temp )\n",
    "    \n",
    "    def find_nearest_clusters( self ): # Findes 2-nearest clusters in each episode.\n",
    "        nearest_clusters = ( -1 , -1 )\n",
    "        min = np.inf\n",
    "        n_clusters = len( self.clusters.keys() )\n",
    "        for number_1 in range( n_clusters -1):\n",
    "            for number_2 in range( number_1+1 , n_clusters ):\n",
    "                temp_dist = self.cluster_distance( number_1 , number_2 )\n",
    "                if temp_dist < min :\n",
    "                    min = temp_dist\n",
    "                    nearest_clusters = ( number_1 , number_2 )\n",
    "        return nearest_clusters\n",
    "\n",
    "    def combine_nearest_clusters( self , nearest ):\n",
    "        first_cluster = nearest[ 0 ]\n",
    "        second_cluster = nearest[ 1 ]\n",
    "        last_cluster = len( self.clusters.keys() )\n",
    "        combined = np.concatenate( (self.clusters[ first_cluster ] , self.clusters[ second_cluster ]) )\n",
    "        self.clusters[ first_cluster ] = combined\n",
    "        if second_cluster < last_cluster - 1 :\n",
    "            for number in range( second_cluster , last_cluster-1 ):\n",
    "                self.clusters[ number ] = self.clusters[ number + 1 ]\n",
    "        del self.clusters[ last_cluster - 1 ]\n",
    "\n",
    "    def assign_label( self , data ):\n",
    "        self.predicted = -1*np.ones(data.shape[0])\n",
    "        for i in self.clusters.keys():\n",
    "            for j in self.clusters[ i ]:\n",
    "                self.predicted[ j ] = i\n",
    "\n",
    "    def average_silhoutte( self , data ): # calculates silhouette score for each set of clusters that made in diffenrent episodes in last 40 episodes.\n",
    "        self.avg_silhoutte_score.append( sklearn.metrics.silhouette_score( data , self.predicted ) )\n",
    "\n",
    "    def run( self , data , n = np.inf ):\n",
    "        self.cal_affinity( data )\n",
    "        size_data = data.shape[ 0 ]\n",
    "        for i in range( size_data ):\n",
    "            self.clusters[ i ] = np.array([ i ]) # setting all datas as different clusters at the begining of function call.\n",
    "        itter = 0\n",
    "        while len( self.clusters ) > self.n_clusters :\n",
    "            if itter == n:\n",
    "                break\n",
    "            nearest_clusters = self.find_nearest_clusters()\n",
    "            self.combine_nearest_clusters( nearest_clusters )\n",
    "            # calculated how much of progress proceeds.\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('\\r')\n",
    "            percent = ((size_data - len( self.clusters ))*100)/(size_data - self.n_clusters )\n",
    "            percent_5  = int(percent//5)\n",
    "            sys.stdout.write(\"[%-20s] %.5f%%\" % ('='*percent_5, percent))        \n",
    "\n",
    "            itter += 1\n",
    "            if len( self.clusters ) < 41 : \n",
    "                self.assign_label( data ) \n",
    "                self.average_silhoutte( data )\n",
    "                self.assigned_labels_all.append( self.predicted )\n",
    "        self.assigned_labels_all.reverse()\n",
    "        self.avg_silhoutte_score.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlxypP4934E2",
    "outputId": "161f578f-7592-4b48-9f9f-85a06c8f26cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================] 100.00000%cp: cannot create regular file 'drive/My Drive/IS/Final/': No such file or directory\n",
      "cp: cannot create regular file 'drive/My Drive/IS/Final/': No such file or directory\n",
      "cp: cannot create regular file 'drive/My Drive/IS/Final/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "agg = Agglomerative( n_clusters = 2 , affinity = 'euclidean' , linkage = 'single' )\n",
    "d = toy_data[0:1000,:]\n",
    "agg.run( d )\n",
    "df = pd.DataFrame( agg.assigned_labels_all )\n",
    "df.to_csv('Toy_1000_all_assiged_labels.csv')\n",
    "!cp Toy_1000_all_assiged_labels.csv \"drive/My Drive/IS/Final/\"\n",
    "df_1 = pd.DataFrame( agg.avg_silhoutte_score )\n",
    "df_1.to_csv('Toy_1000_avg_silhoutte_score.csv')\n",
    "!cp Toy_1000_avg_silhoutte_score.csv \"drive/My Drive/IS/Final/\"\n",
    "df = pd.DataFrame( d )\n",
    "df.to_csv('Toy_trained_data.csv')\n",
    "!cp Toy_trained_data.csv \"drive/My Drive/IS/Final/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "T1mVQ4OU1-HH"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/IS/Final/Toy_1000_all_assiged_labels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a251395f1fb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mToy_1000_all_assiged_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/IS/Final/Toy_1000_all_assiged_labels.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mToy_1000_avg_silhoutte_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/IS/Final/Toy_1000_avg_silhoutte_score.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mToy_trained_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/IS/Final/Toy_trained_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/IS/Final/Toy_1000_all_assiged_labels.csv'"
     ]
    }
   ],
   "source": [
    "Toy_1000_all_assiged_labels = pd.read_csv('/content/drive/My Drive/IS/Final/Toy_1000_all_assiged_labels.csv').to_numpy()\n",
    "Toy_1000_avg_silhoutte_score = pd.read_csv('/content/drive/My Drive/IS/Final/Toy_1000_avg_silhoutte_score.csv').to_numpy()\n",
    "Toy_trained_data = pd.read_csv('/content/drive/My Drive/IS/Final/Toy_trained_data.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "IsU0-4EuGKrs",
    "outputId": "69a53082-21a5-4ee6-dafe-f333e9cefc65"
   },
   "outputs": [],
   "source": [
    "spec_silhouette_avgs = []\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plt.plot(range(1, len(Toy_1000_avg_silhoutte_score[:,1])+1), Toy_1000_avg_silhoutte_score[:,1], '-o', alpha = 0.5, color = 'mediumpurple')\n",
    "ax.set(title='Silhouette Score Agglomerative Clustering in Toy Dataset', ylabel = 'score', xlabel='k');\n",
    "ax.axvline(x = np.argmax(Toy_1000_avg_silhoutte_score[:,1]) + 1, color = 'mediumpurple', label = 'zero eigenvalues', linestyle  ='--')\n",
    "ax.set_xticks(range(1, len(Toy_1000_avg_silhoutte_score[:,1])+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2kfTTRs4uLH"
   },
   "outputs": [],
   "source": [
    "Toy_trained_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "8p4lx2FmliKf",
    "outputId": "a00f52b6-f7bf-4264-e74c-a2a15226639c"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = cm.Purples(Toy_1000_all_assiged_labels[9,1:].astype(float) / np.unique(Toy_1000_all_assiged_labels[9,1:]).shape[0] + 0.5 )\n",
    "ax.scatter(Toy_trained_data[:, 1], Toy_trained_data[:, 2], marker = '.', s = 30, lw = 0, alpha = 0.7, c = colors)\n",
    "centers = [np.mean(Toy_trained_data[:,1:3][Toy_1000_all_assiged_labels[9,1:] == i], axis = 0) for i in range(np.unique(Toy_1000_all_assiged_labels[9,1:]).shape[0])]\n",
    "for i, c in enumerate(centers): ax.scatter(c[0], c[1],  marker='o', c=\"white\", alpha = 1, s = 200, edgecolor='k')\n",
    "for i, c in enumerate(centers): ax.scatter(c[0], c[1], marker='$%d$' % i, alpha = 1, s =50, edgecolor='k')\n",
    "ax.set_title(\"The visualization of the clustered data(from scratch)\")\n",
    "ax.set_xlabel(\"Feature space for the 1st feature\")\n",
    "ax.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQDybzvArQkJ",
    "outputId": "70de87e2-ffb1-4705-c530-e1c1b3b8a9aa"
   },
   "outputs": [],
   "source": [
    "np.random.shuffle( norm_bboxes )\n",
    "agg_2 = Agglomerative( n_clusters = 2 , affinity = 'euclidean' , linkage = 'single' )\n",
    "d_2 = norm_bboxes[0:1000,:]\n",
    "agg_2.run( d_2 )\n",
    "df = pd.DataFrame( agg_2.assigned_labels_all )\n",
    "df.to_csv('BBoxes_1000_all_assiged_labels.csv')\n",
    "!cp BBoxes_1000_all_assiged_labels.csv \"drive/My Drive/IS/Final/\"\n",
    "df_1 = pd.DataFrame( agg_2.avg_silhoutte_score )\n",
    "df_1.to_csv('BBoxes_1000_avg_silhoutte_score.csv')\n",
    "!cp BBoxes_1000_avg_silhoutte_score.csv \"drive/My Drive/IS/Final/\"\n",
    "df_3 = pd.DataFrame( d_2 )\n",
    "df_3.to_csv('BBoxes_trained_data.csv')\n",
    "!cp BBoxes_trained_data.csv \"drive/My Drive/IS/Final/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_TVQtdV501O"
   },
   "outputs": [],
   "source": [
    "BBoxes_1000_all_assiged_labels = pd.read_csv('/content/drive/My Drive/IS/Final/BBoxes_1000_all_assiged_labels.csv').to_numpy()\n",
    "BBoxes_1000_avg_silhoutte_score = pd.read_csv('/content/drive/My Drive/IS/Final/BBoxes_1000_avg_silhoutte_score.csv').to_numpy()\n",
    "BBoxes_trained_data = pd.read_csv('/content/drive/My Drive/IS/Final/BBoxes_trained_data.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "Ip9oP66M1WRt",
    "outputId": "9668fbd4-fbc5-4f42-9643-d66abcb0d541"
   },
   "outputs": [],
   "source": [
    "spec_silhouette_avgs = []\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plt.plot(range(2, len(BBoxes_1000_avg_silhoutte_score[:,1])+2), BBoxes_1000_avg_silhoutte_score[:,1], '-o', alpha = 0.5, color = 'mediumpurple')\n",
    "ax.set(title='Silhouette Score Agglomerative Clustering in Toy Dataset', ylabel = 'score', xlabel='k');\n",
    "ax.axvline(x = np.argmax(BBoxes_1000_avg_silhoutte_score[:,1]) + 2, color = 'mediumpurple', label = 'zero eigenvalues', linestyle  ='--')\n",
    "ax.set_xticks(range(2, len(BBoxes_1000_avg_silhoutte_score[:,1])+2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "QOoPf3eT68l5",
    "outputId": "74ff2ff7-808a-4bad-f322-9d934bbe0241"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = cm.Purples(BBoxes_1000_all_assiged_labels[4,1:].astype(float) / np.unique(BBoxes_1000_all_assiged_labels[4,1:]).shape[0] + 0.5 )\n",
    "ax.scatter(BBoxes_trained_data[:, 1], BBoxes_trained_data[:, 2], marker = '.', s = 30, lw = 0, alpha = 0.7, c = colors)\n",
    "centers = [np.mean(BBoxes_trained_data[:,1:3][BBoxes_1000_all_assiged_labels[4,1:] == i], axis = 0) for i in range(np.unique(BBoxes_1000_all_assiged_labels[4,1:]).shape[0])]\n",
    "for i, c in enumerate(centers): ax.scatter(c[0], c[1],  marker='o', c=\"white\", alpha = 1, s = 200, edgecolor='k')\n",
    "for i, c in enumerate(centers): ax.scatter(c[0], c[1], marker='$%d$' % i, alpha = 1, s =50, edgecolor='k')\n",
    "ax.set_title(\"The visualization of the clustered data(from scratch)\")\n",
    "ax.set_xlabel(\"Feature space for the 1st feature\")\n",
    "ax.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Agglomerative.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
